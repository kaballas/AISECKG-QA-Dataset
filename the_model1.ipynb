{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaballas/AISECKG-QA-Dataset/blob/main/the_model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input Embeddings are the first step to build a transformer.\n",
        "We set up the dimension of the embeddings (d_model),\n",
        "and the vocab size (vocan_size). Then we use an embedding\n",
        "layer from torch.nn and set num_embeddings and embedding_dim\n",
        "\"\"\"\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.voca_size = vocab_size\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multiply the embeddings by the squared root of the d_model (Vaswani et al. 2017).\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Because the attention mechanisms are position invariant, we need\n",
        "to encode the information of word order in some way. The\n",
        "authors came up with positional encoding, which is summed to\n",
        "the original embeddings.\n",
        "\"\"\"\n",
        "class PositionalEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout= nn.Dropout(dropout)\n",
        "\n",
        "        # Create a tensor of zeros to fill in\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a positions vector of shape seq_len. We use .arange()\n",
        "        position = torch.arange(start=0, end=seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        # We create the division term of the formula\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
        "        # Apply sine and cosine. The sine is applied to even numbers; cosine to odd numbers\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Now we need to make it applicable to batches. To do so, we need\n",
        "        # to add an extra dimension in the first position. We do so by using\n",
        "        # .unsqueeze at position 0\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # We need to register the tensor in the buffer of the module (kind of remember it).\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We add the positional encoding to the input tensor. We slice it to match\n",
        "        # the dimensions of the word embedding. Remember the dimensions are: [batch, pos_embedings, dimension]\n",
        "        # We take dim 1 (pos_embeddings) and align them with the dim 1 of x (the\n",
        "        # actual word embeddings). We also make sure to make the positional\n",
        "        # embeddings static (.requires_grad).\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We now design the Layer Normalization. We do this by passing an epsilon value (eps),\n",
        "which is added to avoid 0 division in the normalization operation.\n",
        "\"\"\"\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 10 ** -6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # We set alpha and bias as trainable params\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias  = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get mean and std\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        # Finally use the formula to normalize\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This is the Position Wise Feed-Forward Network. That's just a fancy name\n",
        "for a simple neural network, with two layers and a Relu activation in between.\n",
        "\"\"\"\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff) # This corresponds to W1 and B1 of Vaswani et al. (2017)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model) # This corresponds to W2 and B2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the main function and return [d_model] dim\n",
        "        return self.linear2(self.dropout(nn.ReLU(self.linear1(x))))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This is probably the most important block: the famous Multi-Head Attention.\n",
        "\"\"\"\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # We must be able to send equal dims to the different heads\n",
        "        assert d_model % n_heads == 0, \"Division between d_model and n_heads must be possible.\"\n",
        "\n",
        "        # d_k is the dim that each tensor will have to be parallelized in different heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Set up the weight matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "\n",
        "        # Set Wo, which is [(n_heads * self.d_v), d_model] = [d_model, d_model]\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \"\"\"\n",
        "        COMMON QUESION: If d_k and d_v are the same dimensions,\n",
        "        why do they have different names? d_v is the result of the last\n",
        "        multiplication of the attention formula (which is by V; see\n",
        "        the original paper). However, in practice they are the same.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        # We extact the d_k, which is the last dimension of Q, K, and V\n",
        "        d_k = query.shape[-1]\n",
        "        # We apply the self-attention scores formula. We transpose the last two dims to make the calculation possible\n",
        "        # Transform: [Batch, n_heads, seq_len, d_k] -> [Batch, n_heads, seq_len, seq_len]\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        # This is for the masking\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        # We apply softmax\n",
        "        attention_scores = attention_scores.softmax(dim = -1)\n",
        "        # We add dropout\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # We return a tupple with the attention and the self-attention for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # Project the embeddings into the weight matrices [Batch, seq_len, d_model] -> [Batch, seq_len, d_model]\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        \"\"\"\n",
        "        IMPORTANT:\n",
        "        We need to divide Q, K and V to feed \"pieces\" to different heads (power of parallel processing!)\n",
        "        Transform: [Batch, seq_len, d_model] -> [Batch, seq_len, n_heads, d_k] -> [Batch, n_heads, seq_len x d_k]\n",
        "\n",
        "            - We don't want to split the batches: query.shape[0]\n",
        "            - We don't want to split the sequence: query.shape[1]\n",
        "            - We want to split the d_model (embeddings): self.n_heads, self.d_k\n",
        "            - We want the transposition because we want each head to see the seq_len and d_k\n",
        "\n",
        "        The transposition allows the model to process each head independently across the sequence length. Each head can\n",
        "        focus on different parts of the input sequence, enabling the model to capture various aspects of\n",
        "        the input data in parallel.\n",
        "        \"\"\"\n",
        "        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # here we use the function we previously introduced as a static method\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Here we concatenate the information from the different heads. By multiplying n_heads and d_k,\n",
        "        # we effectively concatenate the d_k elements from each head for every sequence position into a single\n",
        "        # dimension, resulting in a tensor where the information from different heads is concatenated\n",
        "        # for each position in the sequence.\n",
        "        # Transform: [batch, n_heads, seq_len, d_k] -> [batch, seq_len, n_heads, d_k] -> [batch, seq_len, d_model]\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.n_heads * self.d_k)\n",
        "\n",
        "        # Project x it in weights Wo and return it as [batch, seq_len, d_model]\n",
        "        return self.w_o(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Here we will build the residual connection component of the transformer.\n",
        "This will allow a better training and make some 'raw' input flow from layer to layer.\n",
        "\"\"\"\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.normalization = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Normalize x, then pass it through a sublayer (any type), use the dropout term, and finally add x\n",
        "        return x + self.dropout(sublayer(self.normalization(x)))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Here's the encoder block that we will use to create the\n",
        "Encoder object (stacking of Encoder layers)\n",
        "\"\"\"\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        # This is the multi-head attention\n",
        "        self.self_attention_block = self_attention_block\n",
        "        # This is the Point-wise feed forward network\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        # we pack two residual connections in a nn.ModuleList\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # In the first residual connection (idx: 0), we are using MultiHeadAttention, which takes Q, K, V and mask, and\n",
        "        # the residual input. We add both and we pass that result to the second residual connection (idx: 1). This\n",
        "        # makes the same operation but with the FeedForwardBlock.\n",
        "        x = self.residual_connections[0](lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This is how we stack the Encoder block in several layers. This will be\n",
        "the main Encoder object. Because it has to be able to take several Encoder\n",
        "blocks, we use nn.ModuleList as a parameter\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        # We can create as many layers as we want\n",
        "        self.n_layers = n_layers\n",
        "        self.normalization = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # we iterate over n layers\n",
        "        for layer in self.n_layers:\n",
        "            x = layer(x, mask)\n",
        "        # finally we normalize\n",
        "        return self.normalization(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This will be the Decoder block that will allow us to make several layers of it.\n",
        "We introduce cross attention, which is similar to multi-head attention but taken\n",
        "parameters from the encoder.\n",
        "\"\"\"\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock,\n",
        "                 feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        # Initialize all the pieces\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        # Get three residual connections\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_out, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](lambda x: self.self_attention_block(x, encoder_out, encoder_out, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This will be our main Decoder object that we will use to ensemble\n",
        "all layers.\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        # we set the n_layers parameter\n",
        "        self.n_layers = n_layers\n",
        "        self.normalization = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        # we iterate over n layers\n",
        "        for layer in self.n_layers:\n",
        "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
        "        # finally we normalize\n",
        "        return self.normalization(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Final layer to convert logits to a probability distribution over all the vocabulary\n",
        "of the target language (we are building the og transformer)\n",
        "\"\"\"\n",
        "class LastLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.fc(x), dim=-1)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This is the actual beast: the Transformer. We use all the blocks we have been\n",
        "carefully assembling to build the following class\n",
        "\"\"\"\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings,\n",
        "                 tgt_embed: InputEmbeddings, src_pos: PositionalEmbeddings, tgt_pos: PositionalEmbeddings,\n",
        "                 last_linear: LastLinear):\n",
        "        super().__init__()\n",
        "        # Set all the components we will need\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.last_linear = last_linear\n",
        "\n",
        "    # We define an encode() function for the encoder.\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    # We define a decode() function for the decoder.\n",
        "    def decode(self, enc_out, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
        "\n",
        "    # We pass the outputs through the final linear layer\n",
        "    def linear(self, x):\n",
        "        return self.last_linear(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We will finally create a function that allows us to ensemble everything\n",
        "together and create the layers of encoders and decoders. This is what the\n",
        "original paper calls Nx in the figure to the sides of the main figure.\n",
        "\"\"\"\n",
        "def build_transformer_model(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int,\n",
        "                            tgt_seq_len: int, d_model: int= 512, n_layers: int = 6,\n",
        "                            n_heads: int = 8, dropout: float = 0.1, hidden_size: int = 2048):\n",
        "\n",
        "    # Make the embedding layers for input and target\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Make the positional embeddings for input and target\n",
        "    # (in practice you can use the same for both src and tgt)\n",
        "    src_pos = PositionalEmbeddings(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEmbeddings(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(n_layers):\n",
        "        encoder_self_attention = MultiHeadAttentionBlock(d_model, n_heads, dropout)\n",
        "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention, feed_forward, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(n_layers):\n",
        "        decoder_self_attention = MultiHeadAttentionBlock(d_model, n_heads, dropout)\n",
        "        decoder_cross_attention = MultiHeadAttentionBlock(d_model, n_heads, dropout)\n",
        "        feed_forward = FeedForwardBlock(d_model, hidden_size, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention, decoder_cross_attention, feed_forward, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    \"\"\"\n",
        "    Now we pass those layers as the argumemtn to the main objects.\n",
        "    Remember that our main classes for encoder and decoder take\n",
        "    nn.ModuleList as arguments (aka, the layers stacked)\n",
        "    \"\"\"\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    last_layer = LastLinear(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Now, build our model using the transformer class we built\n",
        "    transformer = Transformer(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        src_embed=src_embed,\n",
        "        tgt_embed=tgt_embed,\n",
        "        src_pos=src_pos,\n",
        "        tgt_pos=tgt_pos,\n",
        "        last_linear=last_layer\n",
        "    )\n",
        "\n",
        "    # Now we initialize the parameters with Xavier initialization\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    # FINALLY, return the transformer\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "MEHiZtem3_vm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mR0WR1L740Nl",
        "outputId": "f0625795-ec5d-4ec5-a0c4-795111ad4fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "NL7H6m14EBDO",
        "outputId": "3a99b9bf-73b0-4dbc-b533-4879da92d84b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import math\n",
        "\n",
        "# Define model components\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(nn.ReLU()(self.linear1(x))))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout):\n",
        "        d_k = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        p_attn = scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "        return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores, attn = self.attention(q, k, v, mask, self.dropout)\n",
        "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.w_o(scores)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormalization(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(2)])\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([layer for _ in range(n_layers)])\n",
        "        self.norm = LayerNormalization(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, size, self_attention, cross_attention, feed_forward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention = self_attention\n",
        "        self.cross_attention = cross_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(3)])\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.cross_attention(x, memory, memory, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([layer for _ in range(n_layers)])\n",
        "        self.norm = LayerNormalization(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class LastLinear(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.linear(x), dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, generator):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.generator = generator\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_pos(self.src_embed(src)), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_pos(self.tgt_embed(tgt)), memory, src_mask, tgt_mask)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def generate(self, x):\n",
        "        return self.generator(x)\n",
        "\n",
        "def build_transformer_model(src_vocab_size, tgt_vocab_size, max_len, d_model=512, n_layers=6, n_heads=8, dropout=0.1, d_ff=2048):\n",
        "    attn = MultiHeadAttentionBlock(d_model, n_heads, dropout)\n",
        "    ff = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    position = PositionalEmbeddings(d_model, max_len, dropout)\n",
        "    encoder_layer = EncoderBlock(d_model, attn, ff, dropout)\n",
        "    decoder_layer = DecoderBlock(d_model, attn, attn, ff, dropout)\n",
        "\n",
        "    encoder = Encoder(encoder_layer, n_layers)\n",
        "    decoder = Decoder(decoder_layer, n_layers)\n",
        "\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    model = Transformer(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        src_embed=src_embed,\n",
        "        tgt_embed=tgt_embed,\n",
        "        src_pos=position,\n",
        "        tgt_pos=position,\n",
        "        generator=LastLinear(d_model, tgt_vocab_size)\n",
        "    )\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "## Load dataset and tokenizer\n",
        "#dataset = load_dataset(\"Kaballas/100\", split=\"train\")\n",
        "dataset = load_dataset(\"Kaballas/HRMIS_MASTER\", split=\"train\")\n",
        "\n",
        "# Keep only the columns you want\n",
        "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ['questions', 'answers']])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenization and encoding function\n",
        "def encode(examples):\n",
        "    # Encoding only 'instruction' and 'output'\n",
        "    input_enc = tokenizer(examples['questions'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    output_enc = tokenizer(examples['answers'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    return {\n",
        "        \"input_ids\": input_enc['input_ids'].squeeze(0),\n",
        "        \"attention_mask\": input_enc['attention_mask'].squeeze(0),\n",
        "        \"output_ids\": output_enc['input_ids'].squeeze(0)\n",
        "    }\n",
        "\n",
        "# Apply the encoding function\n",
        "encoded_dataset = dataset.map(encode, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# Define the PyTorch Dataset\n",
        "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encoded_dataset):\n",
        "        self.encoded_dataset = encoded_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(self.encoded_dataset[idx][key]) for key in self.encoded_dataset[idx]}\n",
        "\n",
        "# Create the dataset object\n",
        "full_dataset = Seq2SeqDataset(encoded_dataset)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Parameters\n",
        "src_vocab_size = len(tokenizer)\n",
        "tgt_vocab_size = len(tokenizer)\n",
        "max_len = 128\n",
        "print(f\"Tokenizer src_vocab_size size: {src_vocab_size}\")\n",
        "print(f\"Tokenizer tgt_vocab_size size: {tgt_vocab_size}\")\n",
        "\n",
        "# Build the model\n",
        "model = build_transformer_model(src_vocab_size, tgt_vocab_size, max_len)\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "def create_masks(src, trg):\n",
        "    src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    trg_pad_mask = (trg != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    seq_len = trg.size(1)\n",
        "    nopeak_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).bool()\n",
        "    trg_mask = trg_pad_mask & nopeak_mask\n",
        "    return src_mask, trg_mask\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    accumulation_steps = 4\n",
        "    optimizer.zero_grad()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch['input_ids'].to(device)\n",
        "        trg = batch['output_ids'].to(device)\n",
        "        src_mask, trg_mask = create_masks(src, trg)\n",
        "        enc_output = model.encode(src, src_mask)\n",
        "        dec_output = model.decode(enc_output, src_mask, trg[:, :-1], trg_mask[:, :, :-1, :-1])\n",
        "        output = model.generate(dec_output)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        epoch_loss += loss.item() * accumulation_steps\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch['input_ids'].to(device)\n",
        "            trg = batch['output_ids'].to(device)\n",
        "            src_mask, trg_mask = create_masks(src, trg)\n",
        "            enc_output = model.encode(src, src_mask)\n",
        "            dec_output = model.decode(enc_output, src_mask, trg[:, :-1], trg_mask[:, :, :-1, :-1])\n",
        "            output = model.generate(dec_output)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "patience = 3\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.lr = 0.00001  # Reduce the learning rate\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n",
        "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}')\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.3f}')\n",
        "    scheduler.step(val_loss)\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            #break\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), 'best_model.pt')"
      ],
      "metadata": {
        "id": "4MEvPuAZWmgv",
        "outputId": "923de2bd-9c65-4ae0-f4d8-8c495da5ae2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the latest cached version of the dataset since Kaballas/HRMIS_MASTER couldn't be found on the Hugging Face Hub\n",
            "WARNING:datasets.load:Using the latest cached version of the dataset since Kaballas/HRMIS_MASTER couldn't be found on the Hugging Face Hub\n",
            "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/Kaballas___hrmis_master/default/0.0.0/ea8d856368b3d5ab80c1fed49e21c6335f7b4465 (last modified on Sat Jul 27 06:51:50 2024).\n",
            "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/Kaballas___hrmis_master/default/0.0.0/ea8d856368b3d5ab80c1fed49e21c6335f7b4465 (last modified on Sat Jul 27 06:51:50 2024).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer src_vocab_size size: 30522\n",
            "Tokenizer tgt_vocab_size size: 30522\n",
            "Epoch: 1, Train Loss: 6.136\n",
            "Epoch: 1, Validation Loss: 6.147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "def generate_text(model, tokenizer, input_text, max_len, temperature=0.7):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    input_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    enc_output = model.encode(input_ids, input_mask)\n",
        "    tgt_ids = torch.tensor([[tokenizer.bos_token_id or tokenizer.pad_token_id]]).to(device)\n",
        "    tgt_mask = torch.zeros((1, 1, 1, 1)).to(device)\n",
        "    for i in range(max_len):\n",
        "        dec_output = model.decode(enc_output, input_mask, tgt_ids, tgt_mask)\n",
        "        output = model.generate(dec_output[:, -1, :])\n",
        "        next_word = torch.multinomial(torch.softmax(output / temperature, dim=-1), num_samples=1)\n",
        "        tgt_ids = torch.cat((tgt_ids, next_word), dim=1)\n",
        "        tgt_mask = (tgt_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    return tokenizer.decode(tgt_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the function\n",
        "input_text = \"what data is used for playbacks 1 and 2 in the data environment?\"\n",
        "output_text = generate_text(model, tokenizer, input_text, max_len=128)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "xXakEtiXapiH",
        "outputId": "ca8653bb-11a6-4f5e-9a04-917034f53251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the. be stage the and - byst hr the. employee the central, the the change with the'process the. reporting and the the the time _ to the the purpose.. that the, the. i to a of the cash reports, of.ed the. the the of the in people y in the, of the. and in.g, the. in to the employee. in and is,oit work, to the the is as j of used the purpose, the is -mis. - the purposemis the hr hiring -, leave that identify\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the most of your colab subscription",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}